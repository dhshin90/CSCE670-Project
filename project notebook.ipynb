{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Project: Different Faces of a City\n",
    "\n",
    "\n",
    "Team members: \n",
    "- Tae Jun Jeon\n",
    "- Donghwa Shin\n",
    "- Jingyu Hong\n",
    "- Phakpoom Chinpruttthiwong\n",
    "\n",
    "Project description here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Problem Statement\n",
    "\n",
    "Traditional municipal boundaries set by the government have been used to classify the cities where we live today. Surprisingly, these boundaries have been strong indications of social classes. However, these indications often stay stagnant, and they do not accurately represent the social dynamics that consistently change. To address this problem, we implement a clustering model using the data set from a social network service, Foursquare, to link the geographical locations and its users to better understand a city. Due to the limitation of evaluating clusters, we intend to observe the social activities of a populated city that we are familiar with such as Houston or Austin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Works\n",
    "\n",
    "The methodology of using clustering algorithms to understand social dynamics has existed in previous papers. Lynch approached the problem by observing the structure and function of cities [1], and Milgram approached the problem by highlighting the social interactions by labeling them with local characters [2]. These papers were based on data obtained before the expansion of social media. However, Cranshaw et al. made a map illustrating hidden structures of a city with machine learning and data from a social network service, Foursquare [3]. This new methodology allows us to discern how people in a city actually use the city such as activities people do at night. Here is their good example of New York City. Hence, we have focused on this recent paper that uses the prevalent social data with a novel clustering method, so we can capture the dynamics of our local data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Algorithm\n",
    "\n",
    "To discover clusters in our data set, we implement the variation of spectral clustering method introduced by Ng et al. (2001). Cranshaw et al. improve the previous spectral clustering with the new design of an affinity matrix[3]. We expand the idea by considering the time as a new variable. We suspect that there could be differences in the social pattern when considering types of venues of check-ins from Foursquare. We look at top four common types of venues of check-ins and put them in one time zone. This leads to 4 different time periods: 0-10, 10-18, 19-22,  and 23-0. Thus, the change of social clustering can be observed based on times of check-ins. \n",
    "\n",
    "In short, our goal is to compute affinity matrices for 4 different time periods as described below.\n",
    "\n",
    "Affinity Matrix A = A(i, j) = cosine_sim(i, j) + alpha; if i and j are in the set of n nearest venues (in this work, we use n=20, alpha=0.1). When i and j are not in the nearest venues set, A(i, j) is 0.\n",
    "\n",
    "i, j are vectors of users check-in frequencies of venue i and j respectively.\n",
    "\n",
    "Therefore, the affinity matrix is Nv x Nv where Nv is the number of venues. Most of the matrix entries are zeroes except for 20 entries for each row where the entries are the closest 20 venues.\n",
    "\n",
    "Then we use spectral clustering on these matrices to compute clusters of the cities at each time period and compare the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset comes from Yang et al [4]. This dataset includes long-term global-scale check-in data collected from Foursquare over approximately 18 months from April 2012 to September 2013. \n",
    "\n",
    "#### Data preprocessing and filtering\n",
    "\n",
    "Because the original dataset is enormous and we focus our evaluation only on two cities, Houston, and Austin, we have to filter out data showed in the code below.\n",
    "\n",
    "First we predefine Houston and Austin area as latitude and longitude coordinates as follows:\n",
    "- Austin is a rectangle box from coordinate (30.097979, -98.038402) to (30.51, -97.555003).\n",
    "- Houston is a rectangle box from coordinate (29.488986, -95.810704) to (30.139553, -95.022435)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports and global variables\n",
    "\n",
    "import string\n",
    "import glob, os\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "import operator\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "AUSTIN_LAT_MIN = 30.097979\n",
    "AUSTIN_LAT_MAX = 30.51\n",
    "AUSTIN_LNG_MIN = -98.038402\n",
    "AUSTIN_LNG_MAX = -97.555003\n",
    "    \n",
    "HOUSTON_LAT_MIN = 29.488986\n",
    "HOUSTON_LAT_MAX = 30.139553\n",
    "HOUSTON_LNG_MIN = -95.810704\n",
    "HOUSTON_LNG_MAX = -95.022435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we filter out data to only include venues inside specified coordinates and write them to new files for further processing.\n",
    "\n",
    "The filter_venue function filters venue information data, which includes venue id, latitude, longitude, and type, to only include data from Austin and Houston. There are 8585 venues for Austin, and 10350 venues for Houston after filtering.\n",
    "\n",
    "The filter_checkin function filters check-in information data, which include user id, venue id, and check-in time, to only include data from Austin and Houston. There are 71369 check-ins for Austin, and 65460 check-ins for Houston after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter venue information to only include Austin and Houston venues\n",
    "# Input: Filename of venue data\n",
    "# Return: Venue information {venueid : {\"lat\" : latitude, \"lng\" : longitude}, ...}\n",
    "\n",
    "    \n",
    "def filter_venue(filename):\n",
    "    ret = {}\n",
    "\n",
    "    fp_austin = open(\"austin_venue.txt\", 'a')\n",
    "    fp_houston = open(\"houston_venue.txt\", 'a')\n",
    "    \n",
    "    with open(filename, 'r') as fp:\n",
    "        for line in fp:\n",
    "            arr = re.split(\"\\t\", line)\n",
    "\n",
    "            venueid = arr[0]\n",
    "            lat = float(arr[1])\n",
    "            lng = float(arr[2])\n",
    "\n",
    "            venue_info = {}\n",
    "            venue_info[\"lat\"] = lat\n",
    "            venue_info[\"lng\"] = lng\n",
    "\n",
    "            if (lat > AUSTIN_LAT_MIN and lat < AUSTIN_LAT_MAX) and (lng > AUSTIN_LNG_MIN and lng < AUSTIN_LNG_MAX):\n",
    "                fp_austin.write(line)\n",
    "                ret[venueid] = venue_info\n",
    "\n",
    "            if (lat > HOUSTON_LAT_MIN and lat < HOUSTON_LAT_MAX) and (lng > HOUSTON_LNG_MIN and lng < HOUSTON_LNG_MAX):\n",
    "                fp_houston.write(line)\n",
    "                ret[venueid] = venue_info\n",
    "            \n",
    "    return ret\n",
    "    \n",
    "austin_houston_venue = filter_venue(\"dataset_TIST2015_POIs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter checkin information to only include Austin and Houston checkins\n",
    "# Input: filename of checkin data, venue information {venueid : {\"lat\" : latitude, \"lng\" : longitude}, ...}\n",
    "\n",
    "def filter_checkin(filename, venue):\n",
    "    fp_austin = open(\"austin_checkin.txt\", 'a')\n",
    "    fp_houston = open(\"houston_checkin.txt\", 'a')\n",
    "    \n",
    "    with open(filename, 'r') as fp:\n",
    "        for line in fp:\n",
    "            arr = re.split(\"\\t\", line)\n",
    "\n",
    "            venueid = arr[1]\n",
    "            \n",
    "            if not venue.has_key(venueid):\n",
    "                continue\n",
    "                \n",
    "            venue_info = {}\n",
    "            lat = venue[venueid][\"lat\"]\n",
    "            lng = venue[venueid][\"lng\"]\n",
    "\n",
    "            if (lat > AUSTIN_LAT_MIN and lat < AUSTIN_LAT_MAX) and (lng > AUSTIN_LNG_MIN and lng < AUSTIN_LNG_MAX):\n",
    "                fp_austin.write(line)\n",
    "\n",
    "            if (lat > HOUSTON_LAT_MIN and lat < HOUSTON_LAT_MAX) and (lng > HOUSTON_LNG_MIN and lng < HOUSTON_LNG_MAX):\n",
    "                fp_houston.write(line)\n",
    "\n",
    "filter_checkin(\"dataset_TIST2015_Checkins.txt\", austin_houston_venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we preprocess the n nearest venues of each venue. Because the original algorithm involves implementing kd-tree for efficiency but our implementation is simpler and less efficient, we save the processed data in a file for further usage so that we don't have to recompute it every run. \n",
    "\n",
    "Our method is simple. First, for each venue, we compute the distances of the given venue and the other venues. Then we sort the distances and save the closest 1000 venues. We pick 1000 because the number should cover all venues in the same area, and because there are approximately ten thousands of venues, keeping more than 1000 would make the files too large.\n",
    "\n",
    "The output is then saved in the form ranked by closest distance:\n",
    "\n",
    "venue1: closest_venue1, closest_venue2, ... closest_venue1000\n",
    "\n",
    "and so on ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to compute the distance between two points using lat/long coordinate system\n",
    "# Taken from Micheal Dunn from post http://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "# Input: Two points {\"latitude\": latitude, \"longitude\": longitude}\n",
    "# Return: Distance between two points in float\n",
    "\n",
    "def distance(v1, v2):\n",
    "    lon1 = float(v1[\"longitude\"])\n",
    "    lat1 = float(v1[\"latitude\"])\n",
    "    lon2 = float(v2[\"longitude\"])\n",
    "    lat2 = float(v2[\"latitude\"])\n",
    "\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute 1000 nearest neigbors for later computation\n",
    "# Input: filaname, venue information {venueid : {\"latitiude\" : latitude, \"longitude\" : longitude, \"type\" : type}, ...}\n",
    "# Return: None\n",
    "# Output: To file: venueid:1st_nearest_venueid,2nd_nearest_venueid,...\n",
    "\n",
    "def preprocess_nearest_venues(filename, info):\n",
    "    fp = open(filename, 'a')\n",
    "    l = len(info)\n",
    "    c = 0\n",
    "    n = 1000\n",
    "    for v1 in info.keys():\n",
    "        if c % (l/100) == 0:\n",
    "            print str(c*100.0/float(l))+\"%\"\n",
    "        c += 1\n",
    "        \n",
    "        dist = []\n",
    "        for v2 in info.keys():\n",
    "            if v1 == v2:\n",
    "                continue\n",
    "                \n",
    "            d = distance(info[v1], info[v2])\n",
    "            dist.append((v2, d))\n",
    "        dist.sort(key = lambda elem: elem[1])\n",
    "        s = \"\"\n",
    "        \n",
    "        i = 0\n",
    "        for e in dist:\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            s += e[0]+\",\"\n",
    "            i += 1\n",
    "            \n",
    "        fp.write(v1+\":\"+s[:-1]+\"\\n\")\n",
    "        \n",
    "preprocess_nearest_venues(\"nearest_neighbors_austin_1000.txt\", austin_venue)\n",
    "preprocess_nearest_venues(\"nearest_neighbors_houston_1000.txt\", houston_venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data extraction\n",
    "\n",
    "Here we extract necessary data for clustering. \n",
    "\n",
    "First, we extract check-in information of each venue from the given city. For each venue, it contains a list of check-in time of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get venue's checkin information of given city data\n",
    "# Input: Filename of checkin data\n",
    "# Return: Venue's checkin information {venueid : [{userid : time}, {userid : time}, ...], ... }\n",
    "\n",
    "def get_venue_checkin(filename):\n",
    "    ret = {}\n",
    "    \n",
    "    fp = open(filename, 'r')\n",
    "    lines = fp.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        checkin_time = {}\n",
    "        arr = re.split(\"\\t\", line)\n",
    "        \n",
    "        userid = arr[0]\n",
    "        venueid = arr[1]\n",
    "        time = re.split(' ', arr[2])\n",
    "        hour = time[3][0:2]\n",
    "        \n",
    "        checkin_time[userid] = hour\n",
    "\n",
    "        if not ret.has_key(venueid):\n",
    "            ret[venueid]=[checkin_time]\n",
    "        else:\n",
    "            ret[venueid].append(checkin_time)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "austin_venue_checkin = get_venue_checkin(\"austin_checkin.txt\")\n",
    "houston_venue_checkin = get_venue_checkin(\"houston_checkin.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract venue information. For each venue, it contains latitude, longitude, and venue type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get venue information\n",
    "# Input: Filename of checkin data\n",
    "# Return: Venue information {venueid : {\"latitiude\" : latitude, \"longitude\" : longitude, \"type\" : type}, ...}\n",
    "\n",
    "def get_venue_info(filename):\n",
    "    ret = {}\n",
    "    \n",
    "    fp = open(filename, 'r')\n",
    "    lines = fp.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        arr = re.split(\"\\t\", line)\n",
    "        \n",
    "        venue_info = {}\n",
    "        \n",
    "        venueid = arr[0]\n",
    "        venue_info[\"latitude\"] = arr[1]\n",
    "        venue_info[\"longitude\"] = arr[2]\n",
    "        venue_info[\"type\"] = arr[3]\n",
    "        \n",
    "        ret[venueid] = venue_info\n",
    "\n",
    "    return ret\n",
    "\n",
    "austin_venue = get_venue_info(\"austin_venue.txt\")\n",
    "houston_venue = get_venue_info(\"houston_venue.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our additional contribution to the original work, we have to observe the differences of cities at different periods of time. Thus, we also extact check-in information of each hour in addition to extracting check-in information of the whole day. For each hour, it contains venues, for each venues it contains the frequency of each user visit during the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get checkin frequency of venues based on checkin time in hour\n",
    "# Input: Filename of checkin data, period in hours\n",
    "# Return: Checkin information {hour : {venueid : {userid : checkin_freq, ...}, ...}, ...}\n",
    "\n",
    "def get_checkin_freq_by_hour(filename):\n",
    "    ret = defaultdict(lambda : defaultdict(dict))\n",
    "    \n",
    "    fp = open(filename, 'r')\n",
    "    lines = fp.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        arr = re.split('\\t', line)\n",
    "        \n",
    "        userid = arr[0]\n",
    "        venueid = arr[1]\n",
    "        time = re.split(' ', arr[2])\n",
    "        hour = int(time[3][0:2])\n",
    "        if not ret.has_key(hour):\n",
    "            ret[hour][venueid][userid] = 1\n",
    "        else:\n",
    "            if not ret[hour].has_key(venueid):\n",
    "                ret[hour][venueid][userid] = 1\n",
    "            else:\n",
    "                if not ret[hour][venueid].has_key(userid):\n",
    "                    ret[hour][venueid][userid] = 1\n",
    "                else:\n",
    "                    ret[hour][venueid][userid] += 1\n",
    "                    \n",
    "    return ret\n",
    "    \n",
    "austin_checkin_freq = get_checkin_freq_by_hour(\"austin_checkin.txt\")\n",
    "houston_checkin_freq = get_checkin_freq_by_hour(\"houston_checkin.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_day_checkin_freq(filename):\n",
    "    ret = dict()\n",
    "    \n",
    "    for group in range(0,4):\n",
    "        ret[group]=dict()\n",
    "\n",
    "    for hours,values in filename.items():\n",
    "        for venueid,id_value in values.items():\n",
    "            for userid,count in id_value.items():\n",
    "                    if not ret[0].has_key(venueid):\n",
    "                        ret[0][venueid] = dict()\n",
    "                        ret[0][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[0][venueid].has_key(userid):    \n",
    "                            ret[0][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[0][venueid][userid] += count\n",
    "    return ret\n",
    "\n",
    "all_houston_checkin_freq = all_day_checkin_freq(houston_checkin_freq)\n",
    "all_austin_checkin_freq = all_day_checkin_freq(austin_checkin_freq)\n",
    "# print houston_checkin_freq_categorized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the data for check-in frequencies of each hour, we separate them to two cities: Austin, and Houston.\n",
    "\n",
    "For Houston, we separate time periods into following:\n",
    "- 23-24\n",
    "- 1-9\n",
    "- 10-21\n",
    "- 22\n",
    "\n",
    "For Austin, we separate time periods into the following:\n",
    "- 24-7\n",
    "- 8-9\n",
    "- 10-21\n",
    "- 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorized_hoston_data(filename):\n",
    "    ret = dict()\n",
    "    for group in range(0,4):\n",
    "        ret[group]=dict()\n",
    "    #hours 0 and 23 go to group 0\n",
    "    for hours,values in filename.items():\n",
    "        for venueid,id_value in values.items():\n",
    "            for userid,count in id_value.items():\n",
    "                if hours== 0 or hours==23:\n",
    "                    if not ret[0].has_key(venueid):\n",
    "                        ret[0][venueid]=dict()\n",
    "                        ret[0][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[0][venueid].has_key(userid):    \n",
    "                            ret[0][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[0][venueid][userid] += count\n",
    "                elif 1<= hours and hours <=9:\n",
    "                    if not ret[1].has_key(venueid):\n",
    "                        ret[1][venueid]=dict()\n",
    "                        ret[1][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[1][venueid].has_key(userid):\n",
    "                            ret[1][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[1][venueid][userid] += count\n",
    "                elif 10<= hours and hours <=21:\n",
    "                    if not ret[2].has_key(venueid):\n",
    "                        ret[2][venueid]=dict()\n",
    "                        ret[2][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[2][venueid].has_key(userid):\n",
    "                            ret[2][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[2][venueid][userid] += count\n",
    "                else:\n",
    "                    if not ret[3].has_key(venueid):\n",
    "                        ret[3][venueid]=dict()\n",
    "                        ret[3][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[3][venueid].has_key(userid):\n",
    "                            ret[3][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[3][venueid][userid] += count\n",
    "    return ret\n",
    "\n",
    "houston_checkin_freq_categorized = categorized_hoston_data(houston_checkin_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorized_austin_data(filename):\n",
    "    ret = dict()\n",
    "    for group in range(0,4):\n",
    "        ret[group]=dict()\n",
    "    #hours 0 and 23 go to group 0\n",
    "    for hours,values in filename.items():\n",
    "        for venueid,id_value in values.items():\n",
    "            for userid,count in id_value.items():\n",
    "                if 0<= hours and hours <=7:\n",
    "                    if not ret[0].has_key(venueid):\n",
    "                        ret[0][venueid]=dict()\n",
    "                        ret[0][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[0][venueid].has_key(userid):    \n",
    "                            ret[0][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[0][venueid][userid] += count\n",
    "                elif 8<= hours and hours <=9:\n",
    "                    if not ret[1].has_key(venueid):\n",
    "                        ret[1][venueid]=dict()\n",
    "                        ret[1][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[1][venueid].has_key(userid):\n",
    "                            ret[1][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[1][venueid][userid] += count\n",
    "                elif 10<= hours and hours <=21:\n",
    "                    if not ret[2].has_key(venueid):\n",
    "                        ret[2][venueid]=dict()\n",
    "                        ret[2][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[2][venueid].has_key(userid):\n",
    "                            ret[2][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[2][venueid][userid] += count\n",
    "                else:\n",
    "                    if not ret[3].has_key(venueid):\n",
    "                        ret[3][venueid]=dict()\n",
    "                        ret[3][venueid][userid] = count\n",
    "                    else:\n",
    "                        if not ret[3][venueid].has_key(userid):\n",
    "                            ret[3][venueid][userid] = count\n",
    "                        else:\n",
    "                            ret[3][venueid][userid] += count\n",
    "    return ret\n",
    "\n",
    "austin_checkin_freq_categorized = categorized_austin_data(austin_checkin_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing data\n",
    "**we could move this section to the evaluation too I think\n",
    "\n",
    "Here we create check-in histogram and pie chart of venue type for better understanding the check-in trends.\n",
    "\n",
    "This function creates check-in histogram for each hour of users. We can see the most active time for users here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create hour histogram\n",
    "# Input: Filename of checkin data, period in hours\n",
    "# Return: [freq@0-period, freq@1-2*period, ...]\n",
    "\n",
    "def create_histogram(filename, period):\n",
    "    ret = {}\n",
    "    \n",
    "    fp = open(filename, 'r')\n",
    "    lines = fp.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        arr = re.split('\\t', line)\n",
    "        \n",
    "        time = re.split(' ', arr[2])\n",
    "        hour = int(int(time[3][0:2])/period)\n",
    "        \n",
    "        if not ret.has_key(hour):\n",
    "            ret[hour] = 1\n",
    "        else:\n",
    "            ret[hour] += 1\n",
    "            \n",
    "    sorted_keys = sorted(ret)\n",
    "    \n",
    "    ret_list = []\n",
    "    \n",
    "    for k in sorted_keys:\n",
    "        ret_list.append((k, ret[k]))\n",
    "            \n",
    "    return ret_list\n",
    "\n",
    "austin_histogram = create_histogram(\"austin_checkin.txt\", 1)\n",
    "houston_histogram = create_histogram(\"houston_checkin.txt\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a pie chart for venue's type of each hour. We can see how there is changes to the most popular venue's type of each hour here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create pie chart of venue types for each hour\n",
    "# Input: Checkin information {hour : {venueid : {userid : checkin_freq, ...}, ...}, ...}, \n",
    "#        Venue information {venueid : {\"latitiude\" : latitude, \"longitude\" : longitude, \"type\" : type}, ...},\n",
    "#        period in hours\n",
    "# Return: pie chart {hour : {venue_type : freq, ...}, ...}\n",
    "\n",
    "def create_pie_chart(freq, info):\n",
    "    ret = {}\n",
    "    \n",
    "    for hour, venues in freq.iteritems():\n",
    "        for venueid, users in venues.iteritems():\n",
    "            venue_type = info[venueid][\"type\"]\n",
    "            \n",
    "            checkin_total = 0\n",
    "            for userid, checkin_count in users.iteritems():\n",
    "                checkin_total += checkin_count\n",
    "                \n",
    "            if not ret.has_key(hour):\n",
    "                ret[hour] = {}\n",
    "                ret[hour][venue_type] = checkin_total\n",
    "                \n",
    "            else:\n",
    "                if not ret[hour].has_key(venue_type):\n",
    "                    ret[hour][venue_type] = checkin_total\n",
    "                else:\n",
    "                    ret[hour][venue_type] += checkin_total\n",
    "                    \n",
    "    return ret\n",
    "\n",
    "austin_pie_chart = create_pie_chart(austin_checkin_freq, austin_venue)\n",
    "houston_pie_chart = create_pie_chart(houston_checkin_freq, houston_venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithm\n",
    "\n",
    "After we have extracted necessary information (users check-ins per hour, and venues information), we proceed further to compute the affinity matrix and implement Spectral clustering algorithm.\n",
    "\n",
    "First we normalize the vector to make it a unit vector for further computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for cosine similarity. Create a unit vector.\n",
    "# Input: Vector for user checkin frequency {userid : freq, ...}\n",
    "# Return: Unit vector for user checkin frequency {userid : normalized_freq, ...}\n",
    "\n",
    "def normalized(vect):\n",
    "    ret = {}\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for userid, freq in vect.iteritems():\n",
    "        total += freq**2\n",
    "    \n",
    "    normalizer = math.sqrt(total)\n",
    "    \n",
    "    for userid, freq in vect.iteritems():\n",
    "        ret[userid] = float(float(freq)/float(normalizer))\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we obtain the precomputed nearest venues to compute affinity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the n nearest venues from precomputed nearest venue list file\n",
    "# Input: filename, number of nearest venues\n",
    "# Return: n nearest venues for each venue {venueid : [venueid, ...], ...}\n",
    "\n",
    "def get_nearest_venues(filename, n):\n",
    "    ret = {}\n",
    "\n",
    "    fp = open(filename, 'r')\n",
    "    lines = fp.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        arr = re.split(':', line)\n",
    "        \n",
    "        venueid = arr[0]\n",
    "        nearest_list = re.split(',', arr[1])\n",
    "        \n",
    "        ret[venueid] = nearest_list[0:n]\n",
    "             \n",
    "    return ret\n",
    "\n",
    "austin_nearest = get_nearest_venues(\"nearest_neighbors_austin_1000.txt\", 20)\n",
    "houston_nearest = get_nearest_venues(\"nearest_neighbors_houston_1000.txt\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the cosine similarity and affinity matrix as described in the Core Algorithm section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute cosine similarity between two vectors\n",
    "# Input: Two user checkin frequency vectors {userid : freq, ...}, {userid : freq, ...}\n",
    "# Return: similarity score\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    ret = 0\n",
    "    \n",
    "    v1 = normalized(vec1)\n",
    "    v2 = normalized(vec2)\n",
    "    \n",
    "    common_keys = list(set(v1.keys()) & set(v2.keys()))\n",
    "    \n",
    "    for k in common_keys:\n",
    "        ret += v1[k]*v2[k]\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: {venueid : {userid : checkin_freq, ...}, ...},\n",
    "#        {venueid : [venueid, ...], ...}\n",
    "# Return: Nv x Nv Affinity Matrix; Nv = # venues\n",
    "#         {venueid : {venueid : score, ...}, ...}\n",
    "\n",
    "def create_affinity_matrix(venues, nearest):\n",
    "    alpha = 0.1\n",
    "    \n",
    "    ret = {}\n",
    "    \n",
    "    #l = len(venues)\n",
    "    #c = 0\n",
    "    \n",
    "    for v1 in venues.keys():\n",
    "        ret[v1] = {}\n",
    "        \n",
    "        #if c % (l/100) == 0:\n",
    "        #    print str(c*100.0/float(l))+\"%\"\n",
    "        #c += 1\n",
    "\n",
    "        for v2 in venues.keys():\n",
    "            if v2 in nearest[v1] or v1 in nearest[v2]:\n",
    "                ret[v1][v2] = cosine_sim(venues[v1], venues[v2]) + alpha\n",
    "            else:\n",
    "                ret[v1][v2] = 0\n",
    "        \n",
    "#         for v2 in nearest[v1]:\n",
    "#             ret[v1][v2] = cosine_sim(venues[v1], venues[v2]) + alpha\n",
    "\n",
    "    return ret\n",
    "\n",
    "# houston_affinity = create_affinity_matrix(houston_checkin_freq_categorized[0], houston_nearest)\n",
    "# houston_affinity = create_affinity_matrix(all_houston_checkin_freq[0], houston_nearest)\n",
    "# austin_affinity = create_affinity_matrix(austin_checkin_freq_categorized[3], austin_nearest)\n",
    "austin_affinity = create_affinity_matrix(all_austin_checkin_freq[0], austin_nearest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the affinity matrices, we apply a general version of Spectral clustering algorithm to obtain the final results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General version of Spectral clustering algorithm\n",
    "\n",
    "def spectralClustering(venue, precomputed_affinity, theshold):\n",
    "\n",
    "    affinity_matrix = []\n",
    "\n",
    "    l = len(precomputed_affinity)\n",
    "    c = 0\n",
    "\n",
    "    for k in precomputed_affinity:\n",
    "\n",
    "        if c % (l/100) == 0:\n",
    "            print str(c*100.0/float(l))+\"%\"\n",
    "        c += 1\n",
    "\n",
    "        temp_matrix = []\n",
    "        for k2 in precomputed_affinity[k]:\n",
    "            temp_matrix.append(precomputed_affinity[k][k2])\n",
    "        affinity_matrix.append(np.array(temp_matrix))\n",
    "\n",
    "    affinity_matrix = np.array(affinity_matrix)\n",
    "\n",
    "    print affinity_matrix.shape\n",
    "\n",
    "    #labels = spectral_clustering(graph, n_clusters = 4, eigen_solver = 'arpack')\n",
    "    sc = SpectralClustering(n_clusters = 25, n_jobs = -1, affinity = 'precomputed')\n",
    "    sc.fit(affinity_matrix)\n",
    "\n",
    "    labels = sc.labels_\n",
    "\n",
    "    cluster = {}\n",
    "    c = 0\n",
    "\n",
    "    for v in precomputed_affinity:\n",
    "        cluster[v] = labels[c]\n",
    "        c += 1\n",
    "\n",
    "    cluster_count = {}\n",
    "\n",
    "    for v in venue:\n",
    "        if v in precomputed_affinity:\n",
    "            if cluster_count.get(cluster[v]) == None:\n",
    "                cluster_count[cluster[v]] = 1\n",
    "            else:\n",
    "                cluster_count[cluster[v]] += 1\n",
    "\n",
    "    max_index = max(cluster_count.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    t = []\n",
    "    data = {}\n",
    "\n",
    "    print \"Total # of venues\", len(precomputed_affinity)\n",
    "    print \"Noise cluster: \", max_index\n",
    "    print \"Cluster size: \", cluster_count[max_index]\n",
    "\n",
    "    for v in venue:\n",
    "        if v in precomputed_affinity:\n",
    "             if float(venue[v]['longitude']) > theshold and cluster[v] != max_index:\n",
    "                x.append(venue[v]['longitude'])\n",
    "                y.append(venue[v]['latitude'])\n",
    "                t.append(cluster[v])\n",
    "\n",
    "                if data.get(cluster[v]) == None:\n",
    "                    data.setdefault(cluster[v], [])\n",
    "                temp = {}\n",
    "                temp.setdefault('longitude', venue[v]['longitude'])\n",
    "                temp.setdefault('latitude', venue[v]['latitude'])\n",
    "\n",
    "                data[cluster[v]].append(temp)\n",
    "\n",
    "    #          elif cluster[v] == max_index:\n",
    "    #             print houston_venue[v]['type']\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    t = np.array(t)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(x, y, c = t)\n",
    "    plt.show()\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for k in data:\n",
    "        out.append(data[k])\n",
    "\n",
    "    with open('coords_list.json', 'w+') as json_file:\n",
    "            json_data = json.dumps(out, indent = True)\n",
    "            json_file.write(json_data)\n",
    "\n",
    "\n",
    "    print \"Clustering completed\"\n",
    "    \n",
    "#spectralClustering(houston_venue, houston_affinity, -96.0)\n",
    "#spectralClustering(austin_venue, austin_affinity, -98.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference:\n",
    "\n",
    "[1] Lynch, K, 1992, The image of the city. MIT press\n",
    "\n",
    "[2] Milgram, S, 1977, The individual in a Social World: Essays and Experiments. London: Longman Education \n",
    "\t\n",
    "[3] Cranshaw, J., Schwartz, R., Hong, J.I. and Sadeh, N., 2012, The livehoods project: Utilizing social media to understand the dynamics of a city. International AAAI Conference on Weblogs and Social Media\n",
    "\n",
    "[4] Yang, Dingqi, et al. \"NationTelescope: Monitoring and visualizing large-scale collective behavior in LBSNs.\" Journal of Network and Computer Applications 55 (2015): 170-180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
